# CPSC-354-Report-Week-3-
Public repository for homework assignment 3: Using LLMs for Literature Review

**A Look Into the Relationship of Generative AI and Programming Langauages**
- By Sofia Bejerano

- LLM used: GPT-4o

# Introduction
I started this line of questioning/research for the fact that I was curious of the process of Generative AI. As someone who has created art beforehand, and has seen many opinions circulating Generative AI in relation to art creation, I could not help but be interested in the relationship of progressing programming languages and already standing Generative AI models, such as Dall-E, and what can be expected from the future of these progressions. I asked about the history of programming languages and the creation of generative AI at first to lay a base for my follow up question. Below, I have included the questions I asked and summarized versions of the answers given by GPT, as the original answers are too lenghty to include in full in this report. 

# Question and Answer with GPT-4
**Question:** How did programming langues first create/impact Generative AI softwares such as Dall-E? 

**Answer:** Programming languages played a crucial role in the creation and evolution of Generative AI softwares like Dall-E, laying the foundation for deep learning, efficient computation and large-scale model development. The following points concern how programming languages first impacted the development of such models: 
1. Python and Ecosystem: the most influential language of Generative AI software. It has ease of use and readability as its strengths, impacting early generative AI models (like GANs [Generative Adversial Networks), which are foundational to image generation tasks performed by softwares such as Dall-E. Deep Learning Libraries played a huge role in the rise of deep learning, cemented by the devlopment of libraries like TensorFlow, Pytorch, etc. (PyTorch is what Dall-E was based on, via OpenAI.)
2. C++ and Performance Optimization: C++ played a key role in performance optimization, particularly for low-level operations required to scale models for softwares like Dall-E. C++ is used for many deep learning framework backend operations. It allows the use of GPUs to accelerate training for Generative AI models.
(...)
5. Evolution of Generative Models: programming languages enabled the creation and refinement of Generative AI architectures that paved the way for models like Dall-E. Some examples are: Generative Adversarial Networks, which are a cornerstone for generative models; Variational Autoencoders, implented using Python's deep learning libraries; and Transformers, introduced in 2017, became a game-changer in the field of AI when adapted for vision tasks in models. 

--

**Question:** Are there weaknesses caused by programming languages for generative AI models, such as Dall-E? If any weaknesses exist, what is the cause for them? 

**Answer:** Yes, there are several weaknesses in programming languages that impact the performance, scalability and efficiency of generative AI models like Dall-E. These weaknesses are often the result of the trade-offs between ease of use and performance, the limitations of high-level languages like Python, and challenges related to managing large-scale complex models. Some key weaknesses are: performance overhead in high-level langauges that can lead to bottlenecks, especially wehn handling large-scale models, memory management and garbage collection and the need for fine-grained control over memory allocation and deallocation in generative AI models, dependency on extenral libraries which can introduce inefficiencies when interfacing lower-level hardware, and energy efficiency of large-scale models consuming enourmous computational resources, which leads to high energy consumption. 

--

**Question:** How are new/current/forming developments in programming languages enchance the capabilities of Generative AI systems? 

**Answer:** New and ongoing developments in programming languages are significnatly enhancing the capabilites of Generative AI systems in several key ways. These advancements aim to improve performance, scalability, ease of use, and flexibility in AI development. The nine key ways listed are: improved performance and efficiency, specialized frameworks and libraries, scalability and distributed training, easier model deployment, multi-langauge interoperability, functional and declarative paradigms, AI-specific langauges, enhanced debugging and interpretability, and quantum computing integration. These advancements are making Generative AI systems faster, more efficien and easier to devlop. They are more widely accessible across different platforms and devices, as well. 

-- 

# Conclusion 
Programming langauges have been the base and cornerstone of Generative AI models such as Dall-E. From the creation of such models to future advancements, programming languages such as Python and C++ form the backbone of many such functions. In the future, developers and programmers may want to focus on the weaknesses listed above, as users may run into some of these issues when using such software, and are ultimately issues taht need to addressed to improve such softwares. With improvements in mind, it is hopeful that Generative AI models will be able to be more sophisticated and creative, while aslo being more energy efficient. 

--

# Some References, as Given by Chat GPT-4o: 
"Artificial Intelligence: A Guide for Thinking Humans" by Melanie Mitchell
"Python Machine Learning" by Sebastian Raschka and Vahid Mirjalili
"Green AI: Reducing the Carbon Footprint of Machine Learning" by Arbab Khan
